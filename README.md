# INFO492 Group 2 Project

Team: Brooke Dietmeier, Minnah Tanzeen, Sonoma Miller

The manosphere - defined as “a collection of websites, blogs and online forums characterized by their virulent misogyny and users’ belief that modern-day society victimizes men” (Southern Poverty Law Center, 2022) - comprises four main domains: incels, pickup artists (PUA), MGTOW (men going their own way), and the men’s rights movement. Within these spaces, users often externalize blame - particularly toward women - framing personal dissatisfaction through narratives that normalize misogyny and, in many cases, promote violence. These ideologies are sustained and amplified through feedback mechanisms such as upvotes, engagement metrics, and algorithmic recommendations.

As generative AI systems, especially large language models (LLMs), become increasingly embedded within online platforms, a critical concern emerges: LLM sycophancy - the tendency of AI to uncritically mirror user input. In male-dominated digital spaces like the manosphere, this dynamic raises the possibility that AI tools may inadvertently legitimize or reinforce harmful beliefs rather than disrupting them. Rather than challenging extremist rhetoric, generative AI systems might subtly validate misogynistic content under the guise of neutral engagement, contributing to the resilience and escalation of online echo chambers.

This project explores the research question: How do different generative AI systems respond to coded and explicit misogynistic rhetoric drawn from the manosphere, and to what extent do these systems reinforce, neutralize, or challenge these narratives depending on the prompting persona? To investigate this, we simulate user interactions by developing distinct AI personas modeled after each of the four manosphere subcultures: incels, pickup artists, MGTOW, and men’s rights advocates. These personas allow us to test how various LLMs respond to differing rhetorical framings and user identities, ranging from passive inquiry to overtly misogynistic discourse.
Through this inquiry, we aim to:

1.) Analyze the mechanisms through which misogynistic and extremist rhetoric spreads and is sustained within manosphere communities;


2.) Evaluate how different AI systems interpret both overt and coded forms of misogyny depending on the framing of the prompt and the simulated user identity;


3.) Identify gaps in current AI safety frameworks and propose interventions to reduce the amplification of toxic content.


By examining how generative AI systems respond to personas modeled on real online communities, our research seeks to illuminate the risks, challenges, and potential safeguards necessary for responsible AI deployment in vulnerable and ideologically extreme digital environments.
